{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbafd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import lightning\n",
    "import torch\n",
    "import argparse\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from mlcolvar.core.transform import Statistics, Transform\n",
    "from mlcolvar.cvs import BaseCV\n",
    "from mlcolvar.core import FeedForward, Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb377156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIM_NORMALIZATION(Transform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim = 1,\n",
    "        normalization_factor = 10,\n",
    "    ):\n",
    "        super().__init__(in_features=feature_dim, out_features=feature_dim)\n",
    "        self.register_buffer(\"feature_dim\", torch.tensor(feature_dim))\n",
    "        self.normalization_factor = normalization_factor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.normalize(x, dim=-1) * self.normalization_factor\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Standard single hidden layer MLP with dropout and GELU activations.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, dim_feedforward: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.ff(x)\n",
    "\n",
    "class SAEncoderLayer(nn.Module):\n",
    "    \"\"\"IPA interleaved with layernorm and MLP.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_pair: int,\n",
    "        n_head: int,\n",
    "        dim_feedforward: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = SAAttention(d_model=d_model, d_pair=d_pair, n_head=n_head, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model=d_model, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x1d: torch.Tensor,\n",
    "        x2d: torch.Tensor,\n",
    "        pose: tuple[torch.Tensor, torch.Tensor],\n",
    "        bias: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        x1d = x1d + self.attn(self.norm1(x1d), x2d, pose, bias)\n",
    "        x1d = x1d + self.ffn(self.norm2(x1d))\n",
    "        return x1d\n",
    "    \n",
    "class SAAttention(nn.Module):\n",
    "    \"\"\"DiG version of the invariant point attention module. See AF2 supplement Alg 22.\n",
    "    I believe SA might stand for \"Structural Attention\", see App B.3 in the DiG paper.\n",
    "\n",
    "    The forward pass of this module is identical to IPA as described in Alg 22 in AF2 supplement,\n",
    "    with the following changes:\n",
    "        1. An extra linear map is applied to the pair representation.\n",
    "        2. Dropout is applied to the output. (In AF2 it is applied outside of IPA. This may be\n",
    "            equivalent.)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        d_model: Dimension of attention dot product * number of heads.\n",
    "        d_pair: Dimension of the pair representation.\n",
    "        n_head: Number of attention heads.\n",
    "        dropout: Dropout probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_pair: int, n_head: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if d_model % n_head != 0:\n",
    "            raise ValueError(\"The hidden size is not a multiple of the number of attention heads.\")\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_model // n_head\n",
    "\n",
    "        self.scalar_query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.scalar_key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.scalar_value = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.pair_bias = nn.Linear(d_pair, n_head, bias=False)\n",
    "        self.point_query = nn.Linear(\n",
    "            d_model, n_head * 3 * 4, bias=False\n",
    "        )  # 4 is N_query_points in Alg 22.\n",
    "        self.point_key = nn.Linear(\n",
    "            d_model, n_head * 3 * 4, bias=False\n",
    "        )  # 4 is N_query_points in Alg 22.\n",
    "        self.point_value = nn.Linear(\n",
    "            d_model, n_head * 3 * 8, bias=False\n",
    "        )  # 8 is N_point_values in Alg 22.\n",
    "\n",
    "        self.scalar_weight = 1.0 / math.sqrt(3 * self.d_k)  # Alg 22 line 7, w_L / sqrt(d_k).\n",
    "        self.point_weight = 1.0 / math.sqrt(3 * 4 * 9 / 2)  # Alg 22 line 7, w_C * w_L.\n",
    "        self.trained_point_weight = nn.Parameter(\n",
    "            torch.rand(n_head)\n",
    "        )  # gamma^h, AF2 Supp Section 1.8.2.\n",
    "        self.pair_weight = 1.0 / math.sqrt(3)  # Alg 22 line 7, w_L.\n",
    "\n",
    "        self.pair_value = nn.Linear(\n",
    "            d_pair, d_model, bias=False\n",
    "        )  # NOTE: AF2 IPA does not have this.\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model * 2 + n_head * 8 * 4, d_model, bias=True)  # Alg 22 line 11.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x1d: torch.Tensor,\n",
    "        x2d: torch.Tensor,\n",
    "        pose: tuple[torch.Tensor, torch.Tensor],\n",
    "        bias: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"Forward pass of the SAAttention module.\n",
    "\n",
    "        Args:\n",
    "            x1d: Invariant sequence representation.\n",
    "            x2d: Invariant pair representation.\n",
    "            pose: Tuple of translation and inverse rotation vectors.\n",
    "            bias: Pair bias, used to encode masking.\n",
    "        \"\"\"\n",
    "        T, R = pose[0], pose[1].transpose(\n",
    "            -1, -2\n",
    "        )  # Transpose to go back to rotations from inverse rotations.\n",
    "\n",
    "        # Compute scalar attention queries keys and values.\n",
    "        # Alg 22 line 1, shape [B, L, nhead, C].\n",
    "        q_scalar = self.scalar_query(x1d).reshape(*x1d.shape[:-1], self.n_head, -1)\n",
    "        k_scalar = self.scalar_key(x1d).reshape(*x1d.shape[:-1], self.n_head, -1)\n",
    "        v_scalar = self.scalar_value(x1d).reshape(*x1d.shape[:-1], self.n_head, -1)\n",
    "\n",
    "        # Perform scalar dot product attention.\n",
    "        # Alg 22 line 7, shape [B, nhead, L, L]\n",
    "        scalar_attn = torch.einsum(\"bihc,bjhc->bhij\", q_scalar * self.scalar_weight, k_scalar)\n",
    "\n",
    "        # Compute point attention queries keys and values.\n",
    "        # Alg 22 line 2-3, shape [B, L, nhead, num_points, 3]\n",
    "        q_point_local = self.point_query(x1d).reshape(*x1d.shape[:-1], self.n_head, -1, 3)\n",
    "        k_point_local = self.point_key(x1d).reshape(*x1d.shape[:-1], self.n_head, -1, 3)\n",
    "        v_point_local = self.point_value(x1d).reshape(*x1d.shape[:-1], self.n_head, -1, 3)\n",
    "\n",
    "        def apply_affine(point: torch.Tensor, T: torch.Tensor, R: torch.Tensor):\n",
    "            \"\"\"Apply affine transformation (T, R) to point x. Acts as x -> R @ x + T. This follows\n",
    "            AF2 Supplement Section 1.1.\n",
    "\n",
    "            Args:\n",
    "                point: Point to transform.\n",
    "                T: Translation vector.\n",
    "                R: Rotation matrix.\n",
    "\n",
    "            Returns:\n",
    "                Transformed point.\n",
    "            \"\"\"\n",
    "            return (\n",
    "                torch.matmul(R[:, :, None, None], point.unsqueeze(-1)).squeeze(-1)\n",
    "                + T[:, :, None, None]\n",
    "            )\n",
    "\n",
    "        # Apply the frames to the attention points.\n",
    "        # Alg 22 lines 7 and 10, shape [B, L, nhead, num_points, 3]\n",
    "        q_point_global = apply_affine(q_point_local, T, R)\n",
    "        k_point_global = apply_affine(k_point_local, T, R)\n",
    "        v_point_global = apply_affine(v_point_local, T, R)\n",
    "\n",
    "        # Compute squared distances between transformed points.\n",
    "        # Alg 22 line 7, shape [B, L, L, nhead, num]\n",
    "        point_attn = torch.norm(q_point_global.unsqueeze(2) - k_point_global.unsqueeze(1), dim=-1)\n",
    "        point_weight = self.point_weight * F.softplus(\n",
    "            self.trained_point_weight\n",
    "        )  # w_L * w_C * gamma^h\n",
    "        point_attn = (\n",
    "            -0.5 * point_weight[:, None, None] * torch.sum(point_attn, dim=-1).permute(0, 3, 1, 2)\n",
    "        )\n",
    "\n",
    "        # Alg 22 line 4.\n",
    "        pair_attn = self.pair_weight * self.pair_bias(x2d).permute(0, 3, 1, 2)\n",
    "\n",
    "        # Compute attention logits, Alg 22 line 7.\n",
    "        attn_logits = scalar_attn + point_attn + pair_attn + bias  # [B, nhead, L, L]\n",
    "\n",
    "        # Compute attention weights.\n",
    "        # Alg 22 line 7, shape [B, nhead, L, L]\n",
    "        attn = torch.softmax(attn_logits, dim=-1)\n",
    "\n",
    "        # Alg 22 line 9.\n",
    "        out_scalar = torch.einsum(\"bhij,bjhc->bihc\", attn, v_scalar)\n",
    "        out_scalar = out_scalar.reshape(*out_scalar.shape[:2], -1)\n",
    "\n",
    "        # Alg 22 line 10.\n",
    "        with torch.amp.autocast(\"cuda\", enabled=False):\n",
    "            out_point_global = torch.einsum(\n",
    "                \"bhij,bjhcp->bihcp\", attn.float(), v_point_global.float()\n",
    "            )\n",
    "        # Inverse affine transformation, as per Alg 22 line 10, and AF2 Supplement Section 1.1.\n",
    "        out_point_local = torch.matmul(\n",
    "            R.transpose(-1, -2)[:, :, None, None],\n",
    "            (out_point_global - T[:, :, None, None]).unsqueeze(-1),\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Alg 22 line 11.\n",
    "        out_point_norm = torch.norm(out_point_local, dim=-1)\n",
    "        out_point_norm = out_point_norm.reshape(*out_point_norm.shape[:2], -1)\n",
    "        out_point_local = out_point_local.reshape(*out_point_local.shape[:2], -1)\n",
    "\n",
    "        # NOTE: AF2 IPA does not project x2d as in here, i.e., v_pair = x2d in AF2.\n",
    "        v_pair = self.pair_value(x2d).reshape(*x2d.shape[:-1], self.n_head, -1)\n",
    "\n",
    "        # Alg 22 line 8.\n",
    "        out_pair = torch.einsum(\"bhij,bijhc->bihc\", attn, v_pair)\n",
    "        out_pair = out_pair.reshape(*out_pair.shape[:2], -1)\n",
    "\n",
    "        # Alg 22 line 11.\n",
    "        out_feat = torch.cat([out_scalar, out_point_local, out_pair, out_point_norm], dim=-1)\n",
    "\n",
    "        # NOTE: AF2 includes dropout outside IPA, not inside. See AF2 Alg 22 line 6.\n",
    "        x = self.dropout(self.fc_out(out_feat))\n",
    "        return x  # [B, L, C]\n",
    "    \n",
    "\n",
    "class MLCV_TRANSFERABLE(BaseCV, lightning.LightningModule):\n",
    "    \"\"\"\n",
    "    Transferable MLCV model that can handle variable protein sizes.\n",
    "    \n",
    "    This model uses a structure-aware attention mechanism (SAEncoderLayer) \n",
    "    to process either pairwise distance matrices or 3D coordinates of different \n",
    "    sizes and output fixed-size collective variables, making it transferable \n",
    "    across proteins.\n",
    "    \n",
    "    Key features:\n",
    "    - Handles variable input sizes (different protein lengths)\n",
    "    - Supports both pairwise distances [B, n_pairs] and 3D coordinates [B, N, 3] as input\n",
    "    - For coordinates: can either convert to distances first or process directly\n",
    "    - Uses position encoding for distance matrix entries or sequence positions\n",
    "    - Employs attention mechanism for sequence-length invariance\n",
    "    - Produces fixed-size output regardless of input protein size\n",
    "    \"\"\"\n",
    "    BLOCKS = [\"norm_in\", \"encoder\",]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        mlcv_dim: int = 2,\n",
    "        d_model: int = 256,\n",
    "        d_pair: int = 128, \n",
    "        n_head: int = 8,\n",
    "        dim_feedforward: int = 512,\n",
    "        n_layers: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "        max_seq_len: int = 500,\n",
    "        dim_normalization: bool = False,\n",
    "        normalization_factor: float = 1.0,\n",
    "        input_type: str = \"distances\",\n",
    "        coordinate_processing: str = \"to_distances\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Input dimension (ignored for distances, used for validation for coordinates)\n",
    "            mlcv_dim: Output dimension of the collective variable\n",
    "            d_model: Model dimension for attention layers\n",
    "            d_pair: Pair representation dimension\n",
    "            n_head: Number of attention heads\n",
    "            dim_feedforward: Feedforward network dimension\n",
    "            n_layers: Number of SA encoder layers\n",
    "            dropout: Dropout probability\n",
    "            max_seq_len: Maximum sequence length for position encoding\n",
    "            dim_normalization: Whether to apply dimension normalization\n",
    "            normalization_factor: Factor for dimension normalization\n",
    "            input_type: Type of input data - \"distances\" for pairwise distances or \"coordinates\" for 3D coordinates\n",
    "            coordinate_processing: How to process coordinates - \"to_distances\" or \"direct\" (only used when input_type=\"coordinates\")\n",
    "        \"\"\"\n",
    "        super().__init__(in_features=input_dim, out_features=mlcv_dim)\n",
    "        \n",
    "        # Validate input_type and coordinate_processing\n",
    "        if input_type not in [\"distances\", \"coordinates\"]:\n",
    "            raise ValueError(f\"input_type must be 'distances' or 'coordinates', got {input_type}\")\n",
    "        \n",
    "        if coordinate_processing not in [\"to_distances\", \"direct\"]:\n",
    "            raise ValueError(f\"coordinate_processing must be 'to_distances' or 'direct', got {coordinate_processing}\")\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.mlcv_dim = mlcv_dim\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.input_type = input_type\n",
    "        self.coordinate_processing = coordinate_processing\n",
    "        \n",
    "        # Input preprocessing layers\n",
    "        self.distance_embedding = torch.nn.Linear(1, d_model)\n",
    "        \n",
    "        # Position encoding for distance matrix indices\n",
    "        self.pos_encoding = PositionalEncodingMatrix(d_model, max_seq_len)\n",
    "        \n",
    "        # Initial projection to pair dimension \n",
    "        self.pair_projection = torch.nn.Linear(d_model, d_pair)\n",
    "        \n",
    "        # Structure-aware encoder layers\n",
    "        self.encoder_layers = torch.nn.ModuleList([\n",
    "            SAEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                d_pair=d_pair, \n",
    "                n_head=n_head,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Global pooling and output projection\n",
    "        self.global_pool = torch.nn.AdaptiveAvgPool1d(1)  # Pool over sequence length\n",
    "        self.output_projection = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, dim_feedforward),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(dim_feedforward, mlcv_dim)\n",
    "        )\n",
    "        \n",
    "        # Optional dimension normalization\n",
    "        if dim_normalization:\n",
    "            self.postprocessing = DIM_NORMALIZATION(\n",
    "                feature_dim=mlcv_dim,\n",
    "                normalization_factor=normalization_factor,\n",
    "            )\n",
    "    \n",
    "    def coordinates_to_distances(self, coordinates: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert 3D coordinates to pairwise distances.\n",
    "        \n",
    "        Args:\n",
    "            coordinates: [B, N, 3] 3D coordinates\n",
    "            \n",
    "        Returns:\n",
    "            distances: [B, n_pairs] pairwise distances where n_pairs = N * (N-1) / 2\n",
    "        \"\"\"\n",
    "        batch_size, n_residues, coord_dim = coordinates.shape\n",
    "        \n",
    "        if coord_dim != 3:\n",
    "            raise ValueError(f\"Expected coordinates with shape [B, N, 3], got shape {coordinates.shape}\")\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        # coordinates: [B, N, 3]\n",
    "        # Expand dimensions for broadcasting: [B, N, 1, 3] and [B, 1, N, 3]\n",
    "        coord_i = coordinates.unsqueeze(2)  # [B, N, 1, 3]\n",
    "        coord_j = coordinates.unsqueeze(1)  # [B, 1, N, 3]\n",
    "        \n",
    "        # Compute squared distances: [B, N, N]\n",
    "        dist_matrix = torch.norm(coord_i - coord_j, dim=-1)\n",
    "        \n",
    "        # Extract upper triangular part (excluding diagonal)\n",
    "        distances = []\n",
    "        for i in range(n_residues):\n",
    "            for j in range(i + 1, n_residues):\n",
    "                distances.append(dist_matrix[:, i, j])\n",
    "        \n",
    "        # Stack to get [B, n_pairs]\n",
    "        distances = torch.stack(distances, dim=1)\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def cad_to_sequence_representation(self, cad_distances: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Convert pairwise CA distances to sequence representation for SA layers.\n",
    "        \n",
    "        Args:\n",
    "            cad_distances: [B, n_pairs] where n_pairs = n_residues * (n_residues - 1) / 2\n",
    "            \n",
    "        Returns:\n",
    "            x1d: [B, L, d_model] sequence representation\n",
    "            x2d: [B, L, L, d_pair] pair representation  \n",
    "            mask: [B, L] sequence mask\n",
    "        \"\"\"\n",
    "        batch_size = cad_distances.shape[0]\n",
    "        n_pairs = cad_distances.shape[1]\n",
    "        \n",
    "        # Reconstruct sequence length from number of pairs\n",
    "        # n_pairs = n * (n-1) / 2, solve for n\n",
    "        n_residues = int((1 + math.sqrt(1 + 8 * n_pairs)) / 2)\n",
    "        \n",
    "        # Embed distance values\n",
    "        dist_embedded = self.distance_embedding(cad_distances.unsqueeze(-1))  # [B, n_pairs, d_model]\n",
    "        \n",
    "        # Create pairwise distance matrix [B, L, L, d_model]\n",
    "        distance_matrix = torch.zeros(batch_size, n_residues, n_residues, self.d_model, \n",
    "                                    device=cad_distances.device, dtype=cad_distances.dtype)\n",
    "        \n",
    "        # Fill upper triangular part\n",
    "        idx = 0\n",
    "        for i in range(n_residues):\n",
    "            for j in range(i + 1, n_residues):\n",
    "                distance_matrix[:, i, j] = dist_embedded[:, idx]\n",
    "                distance_matrix[:, j, i] = dist_embedded[:, idx]  # Symmetric\n",
    "                idx += 1\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_encoded = self.pos_encoding(n_residues)  # [L, L, d_model]\n",
    "        distance_matrix = distance_matrix + pos_encoded.unsqueeze(0)\n",
    "        \n",
    "        # Create sequence representation by averaging over pairs\n",
    "        x1d = distance_matrix.mean(dim=2)  # [B, L, d_model]\n",
    "        \n",
    "        # Create pair representation\n",
    "        x2d = self.pair_projection(distance_matrix)  # [B, L, L, d_pair]\n",
    "        \n",
    "        # Create mask (all positions are valid for distance matrices)\n",
    "        mask = torch.ones(batch_size, n_residues, device=cad_distances.device, dtype=torch.bool)\n",
    "        \n",
    "        return x1d, x2d, mask\n",
    "    \n",
    "    def forward(self, input_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of transferable MLCV.\n",
    "        \n",
    "        Args:\n",
    "            input_data: Either [B, n_pairs] pairwise distances or [B, N, 3] 3D coordinates\n",
    "            \n",
    "        Returns:\n",
    "            cv: [B, mlcv_dim] collective variables\n",
    "        \"\"\"\n",
    "        # Process input based on type and processing method\n",
    "        if self.input_type == \"coordinates\":\n",
    "            # Input is 3D coordinates [B, N, 3]\n",
    "            if len(input_data.shape) != 3 or input_data.shape[-1] != 3:\n",
    "                raise ValueError(f\"For input_type='coordinates', expected shape [B, N, 3], got {input_data.shape}\")\n",
    "            \n",
    "            if self.coordinate_processing == \"direct\":\n",
    "                # Process coordinates directly without converting to distances\n",
    "                x1d, x2d, mask = self.coordinates_to_sequence_representation(input_data)\n",
    "            elif self.coordinate_processing == \"to_distances\":\n",
    "                # Convert to distances first, then process\n",
    "                cad_distances = self.coordinates_to_distances(input_data)\n",
    "                x1d, x2d, mask = self.cad_to_sequence_representation(cad_distances)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown coordinate_processing: {self.coordinate_processing}\")\n",
    "                \n",
    "        elif self.input_type == \"distances\":\n",
    "            # Input is already distances [B, n_pairs]\n",
    "            if len(input_data.shape) != 2:\n",
    "                raise ValueError(f\"For input_type='distances', expected shape [B, n_pairs], got {input_data.shape}\")\n",
    "            x1d, x2d, mask = self.cad_to_sequence_representation(input_data)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown input_type: {self.input_type}\")\n",
    "        \n",
    "        # Create dummy pose (identity rotations and zero translations)\n",
    "        batch_size, seq_len = x1d.shape[:2]\n",
    "        device = x1d.device\n",
    "        \n",
    "        dummy_translations = torch.zeros(batch_size, seq_len, 3, device=device)\n",
    "        dummy_rotations = torch.eye(3, device=device).unsqueeze(0).unsqueeze(0).repeat(batch_size, seq_len, 1, 1)\n",
    "        pose = (dummy_translations, dummy_rotations)\n",
    "        \n",
    "        # Create attention bias (no masking for distance matrices)\n",
    "        bias = torch.zeros(batch_size, 1, seq_len, seq_len, device=device)  # [B, 1, L, L]\n",
    "        \n",
    "        # Apply SA encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x1d = layer(x1d, x2d, pose, bias)\n",
    "        \n",
    "        # Global pooling over sequence length\n",
    "        # x1d: [B, L, d_model] -> [B, d_model, L] -> [B, d_model, 1] -> [B, d_model]\n",
    "        pooled = self.global_pool(x1d.transpose(1, 2)).squeeze(-1)\n",
    "        \n",
    "        # Output projection\n",
    "        cv = self.output_projection(pooled)  # [B, mlcv_dim]\n",
    "        \n",
    "        # Apply post-processing if available\n",
    "        if hasattr(self, 'postprocessing') and self.postprocessing is not None:\n",
    "            cv = self.postprocessing(cv)\n",
    "            \n",
    "        return cv\n",
    "\n",
    "\n",
    "class PositionalEncodingMatrix(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding for pairwise distance matrices.\n",
    "    \n",
    "    Encodes the relative positions (i, j) in the distance matrix \n",
    "    to help the model understand spatial relationships.\n",
    "    \n",
    "    Optimized version using vectorized operations instead of nested loops.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix [max_len, max_len, d_model]\n",
    "        pe = torch.zeros(max_len, max_len, d_model)\n",
    "        \n",
    "        # Create position indices\n",
    "        positions_i = torch.arange(max_len, dtype=torch.float).unsqueeze(1).expand(max_len, max_len)  # [max_len, max_len]\n",
    "        positions_j = torch.arange(max_len, dtype=torch.float).unsqueeze(0).expand(max_len, max_len)  # [max_len, max_len]\n",
    "        rel_distances = torch.abs(positions_i - positions_j)  # [max_len, max_len]\n",
    "        \n",
    "        # Create dimension indices for vectorized computation\n",
    "        dim_indices = torch.arange(0, d_model, 4, dtype=torch.float)  # [d_model//4]\n",
    "        \n",
    "        # Vectorized sinusoidal encoding\n",
    "        for idx, k in enumerate(dim_indices):\n",
    "            k = int(k)\n",
    "            if k < d_model:\n",
    "                # Position i encoding\n",
    "                div_term_i = 10000 ** (k / d_model)\n",
    "                pe[:, :, k] = torch.sin(positions_i / div_term_i)\n",
    "                \n",
    "                if k + 1 < d_model:\n",
    "                    pe[:, :, k + 1] = torch.cos(positions_i / div_term_i)\n",
    "                \n",
    "                if k + 2 < d_model:\n",
    "                    # Position j encoding  \n",
    "                    div_term_j = 10000 ** ((k + 2) / d_model)\n",
    "                    pe[:, :, k + 2] = torch.sin(positions_j / div_term_j)\n",
    "                \n",
    "                if k + 3 < d_model:\n",
    "                    # Relative distance encoding\n",
    "                    div_term_rel = 10000 ** ((k + 3) / d_model)\n",
    "                    pe[:, :, k + 3] = torch.cos(rel_distances / div_term_rel)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: Sequence length\n",
    "            \n",
    "        Returns:\n",
    "            Positional encoding matrix [seq_len, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        return self.pe[:seq_len, :seq_len, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5ea162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['norm_in.mean', 'norm_in.range', 'encoder.nn.0.weight', 'encoder.nn.0.bias', 'encoder.nn.3.weight', 'encoder.nn.3.bias', 'encoder.nn.6.weight', 'encoder.nn.6.bias'])\n"
     ]
    }
   ],
   "source": [
    "date = \"0816_171833\"\n",
    "ckpt_path = f\"/home/shpark/prj-mlcv/lib/bioemu/model/{date}/checkpoint_800.pt\"\n",
    "mlcv_model_ckpt = torch.load(ckpt_path)['mlcv_state_dict']\n",
    "print(mlcv_model_ckpt.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172d84e",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7792f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIM_NORMALIZATION(Transform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim = 1,\n",
    "        normalization_factor = 10,\n",
    "    ):\n",
    "        super().__init__(in_features=feature_dim, out_features=feature_dim)\n",
    "        self.register_buffer(\"feature_dim\", torch.tensor(feature_dim))\n",
    "        self.normalization_factor = normalization_factor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.normalize(x, dim=-1) * self.normalization_factor\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MLCV(BaseCV, lightning.LightningModule):\n",
    "    BLOCKS = [\"norm_in\", \"encoder\",]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mlcv_dim: int,\n",
    "        dim_normalization: bool,\n",
    "        encoder_layers: list,\n",
    "        normalization_factor: float = 1.0,\n",
    "        options: dict = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(in_features=encoder_layers[0], out_features=encoder_layers[-1], **kwargs)\n",
    "        # ======= OPTIONS =======\n",
    "        options = self.parse_options(options)\n",
    "        \n",
    "        # ======= BLOCKS =======\n",
    "        # initialize norm_in\n",
    "        o = \"norm_in\"\n",
    "        if (options[o] is not False) and (options[o] is not None):\n",
    "            self.norm_in = Normalization(self.in_features, **options[o])\n",
    "\n",
    "        # initialize encoder\n",
    "        o = \"encoder\"\n",
    "        self.encoder = FeedForward(encoder_layers, **options[o])\n",
    "        if dim_normalization and mlcv_dim > 1:\n",
    "            self.postprocessing = DIM_NORMALIZATION(\n",
    "                feature_dim=mlcv_dim,\n",
    "                normalization_factor=normalization_factor,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276bbda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLCV(\n",
       "  (norm_in): Normalization(in_features=45, out_features=45, mode=mean_std)\n",
       "  (encoder): FeedForward(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=45, out_features=100, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (4): Tanh()\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=100, out_features=1, bias=True)\n",
       "      (7): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 45\n",
    "mlcv_dim = 1\n",
    "dim_normalization = True\n",
    "normalization_factor = 1.0\n",
    "\n",
    "encoder_layers = [input_dim, 100, 100, mlcv_dim]\n",
    "options = {\n",
    "\t\"encoder\": {\n",
    "\t\t\"activation\": \"tanh\",\n",
    "\t\t\"dropout\": [0.1, 0.1, 0.1]\n",
    "\t},\n",
    "\t\"norm_in\": {\n",
    "\t},\n",
    "}\n",
    "mlcv_model = MLCV(\n",
    "\tmlcv_dim = mlcv_dim,\n",
    "\tdim_normalization = dim_normalization,\n",
    "\tnormalization_factor = normalization_factor,\n",
    "\toptions = options,\n",
    "\tencoder_layers = encoder_layers,\n",
    ")\n",
    "mlcv_model.load_state_dict(mlcv_model_ckpt)\n",
    "mlcv_model.eval()\n",
    "mlcv_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea3606",
   "metadata": {},
   "source": [
    "## Transferable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3715787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLCV_TRANSFERABLE(\n",
      "  (distance_embedding): Linear(in_features=1, out_features=256, bias=True)\n",
      "  (pos_encoding): PositionalEncodingMatrix()\n",
      "  (pair_projection): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-2): 3 x SAEncoderLayer(\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SAAttention(\n",
      "        (scalar_query): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (scalar_key): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (scalar_value): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (pair_bias): Linear(in_features=128, out_features=8, bias=False)\n",
      "        (point_query): Linear(in_features=256, out_features=96, bias=False)\n",
      "        (point_key): Linear(in_features=256, out_features=96, bias=False)\n",
      "        (point_value): Linear(in_features=256, out_features=192, bias=False)\n",
      "        (pair_value): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (fc_out): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (4): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (output_projection): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlcv_model = MLCV_TRANSFERABLE(\n",
    "\tmlcv_dim=1,\n",
    "\tdim_normalization=False,\n",
    "\tnormalization_factor=1,\n",
    "\tinput_dim=45,\n",
    "\tinput_type=\"distances\",\n",
    "\tcoordinate_processing=\"to_distances\"\n",
    ")\n",
    "print(mlcv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a71cc5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLCV_TRANSFERABLE(\n",
       "  (distance_embedding): Linear(in_features=1, out_features=256, bias=True)\n",
       "  (pos_encoding): PositionalEncodingMatrix()\n",
       "  (pair_projection): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-2): 3 x SAEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SAAttention(\n",
       "        (scalar_query): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (scalar_key): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (scalar_value): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (pair_bias): Linear(in_features=128, out_features=8, bias=False)\n",
       "        (point_query): Linear(in_features=256, out_features=96, bias=False)\n",
       "        (point_key): Linear(in_features=256, out_features=96, bias=False)\n",
       "        (point_value): Linear(in_features=256, out_features=192, bias=False)\n",
       "        (pair_value): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (fc_out): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): FeedForward(\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (output_projection): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlcv_model.load_state_dict(mlcv_model_ckpt)\n",
    "mlcv_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e4136",
   "metadata": {},
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784db9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([534743, 45])\n"
     ]
    }
   ],
   "source": [
    "molecule = \"CLN025\"\n",
    "\n",
    "projection_data_path = f\"/home/shpark/prj-mlcv/lib/DESRES/DESRES-Trajectory_{molecule}-0-protein/{molecule}-0-cad.pt\"\n",
    "projection_data = torch.load(projection_data_path)\n",
    "print(projection_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7916c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 534743\n",
      "Batch size: 10000\n",
      "Number of batches: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:03<00:00, 15.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final CV shape: (534743, 1)\n",
      "CV value range: -19.6067 to 20.0193\n",
      "CV mean: -9.8456, std: 15.5492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for batch processing\n",
    "mlcv_model = mlcv_model.to(\"cuda\")\n",
    "projection_data = projection_data.to(\"cuda\")\n",
    "batch_size = 10000  # Adjust batch size based on your memory constraints\n",
    "dataset = TensorDataset(projection_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Total samples: {len(projection_data)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Process data in batches and collect CV values\n",
    "# Fix: CV output should be (n_samples, mlcv_dim) not (n_samples, input_dim)\n",
    "cv_values = np.zeros((projection_data.shape[0], 1))  # Shape: (534743, 1)\n",
    "\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for batch_idx, (batch_data,) in enumerate(tqdm(dataloader)):\n",
    "        batch_cv = mlcv_model(batch_data)\n",
    "        batch_cv_np = batch_cv.detach().cpu().numpy()\n",
    "        cv_values[batch_idx * batch_size:(batch_idx + 1) * batch_size] = batch_cv_np\n",
    "\n",
    "print(f\"\\nFinal CV shape: {cv_values.shape}\")\n",
    "print(f\"CV value range: {cv_values.min().item():.4f} to {cv_values.max().item():.4f}\")\n",
    "print(f\"CV mean: {cv_values.mean().item():.4f}, std: {cv_values.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4585f38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': tensor([-9.8456]),\n",
       " 'std': tensor([15.5492]),\n",
       " 'min': tensor([-19.6067], dtype=torch.float64),\n",
       " 'max': tensor([20.0193], dtype=torch.float64)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = Statistics(torch.from_numpy(cv_values)).to_dict()\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "547f9ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.113712310791016\n"
     ]
    }
   ],
   "source": [
    "def sanitize_range(range_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Sanitize range tensor to avoid division by zero\"\"\"\n",
    "    if (range_tensor < 1e-6).nonzero().sum() > 0:\n",
    "        print(\n",
    "            \"[Warning] Normalization: the following features have a range of values < 1e-6:\",\n",
    "            (range_tensor < 1e-6).nonzero(),\n",
    "        )\n",
    "    range_tensor[range_tensor < 1e-6] = 1.0\n",
    "    return range_tensor\n",
    "\n",
    "class PostProcess(Transform):\n",
    "    \"\"\"Post-processing module for MLCV normalization and sign flipping\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        stats=None,\n",
    "        reference_frame_cv=None,\n",
    "        feature_dim=1,\n",
    "    ):\n",
    "        super().__init__(in_features=feature_dim, out_features=feature_dim)\n",
    "        self.register_buffer(\"mean\", torch.zeros(feature_dim))\n",
    "        self.register_buffer(\"range\", torch.ones(feature_dim))\n",
    "        \n",
    "        if stats is not None:\n",
    "            min_val = stats[\"min\"]\n",
    "            max_val = stats[\"max\"]\n",
    "            self.mean = (max_val + min_val) / 2.0\n",
    "            range_val = (max_val - min_val) / 2.0\n",
    "            self.range = sanitize_range(range_val)\n",
    "        \n",
    "        if reference_frame_cv is not None:\n",
    "            self.register_buffer(\n",
    "                \"flip_sign\",\n",
    "                torch.ones(1) * -1 if reference_frame_cv < 0 else torch.ones(1)\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"flip_sign\", torch.ones(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.sub(self.mean).div(self.range)\n",
    "        x = x * self.flip_sign\n",
    "        return x\n",
    "\n",
    "\n",
    "reference_frame_path = f\"/home/shpark/prj-mlcv/lib/DESRES/data/CLN025/6bond.pdb\"\n",
    "ref_traj = md.load(reference_frame_path)\n",
    "ref_pos = ref_traj.xyz[0]\n",
    "ref_ca_pos = ref_pos[ref_traj.topology.select('name CA')]\n",
    "ref_distances = torch.cdist(torch.from_numpy(ref_ca_pos), torch.from_numpy(ref_ca_pos), p=2)\n",
    "n = ref_distances.shape[0]\n",
    "i, j = torch.triu_indices(n, n, offset=1)\n",
    "ref_cad = ref_distances[i, j].unsqueeze(0).to(mlcv_model.device)\n",
    "reference_frame_cv = mlcv_model(ref_cad).item()\n",
    "print(reference_frame_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "032584bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing = PostProcess(\n",
    "\tstats=stats,\n",
    "\treference_frame_cv=reference_frame_cv,\n",
    "\tfeature_dim=cv_values.shape[1]\n",
    ").to(\"cuda\")\n",
    "mlcv_model.postprocessing = postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e3de743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:03<00:00, 14.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final CV shape: (534743, 1)\n",
      "CV value range: -1.0000 to 1.0000\n",
      "CV mean: 0.5073, std: 0.7848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cv_values_normalized = np.zeros((cv_values.shape[0], 1))  # Shape: (534743, 1)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for batch_idx, (batch_data,) in enumerate(tqdm(dataloader)):\n",
    "        batch_cv = mlcv_model(batch_data)\n",
    "        batch_cv_np = batch_cv.detach().cpu().numpy()\n",
    "        cv_values_normalized[batch_idx * batch_size:(batch_idx + 1) * batch_size] = batch_cv_np\n",
    "\n",
    "print(f\"\\nFinal CV shape: {cv_values_normalized.shape}\")\n",
    "print(f\"CV value range: {cv_values_normalized.min().item():.4f} to {cv_values_normalized.max().item():.4f}\")\n",
    "print(f\"CV mean: {cv_values_normalized.mean().item():.4f}, std: {cv_values_normalized.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d1453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "mlcv_model.trainer = Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    "    enable_model_summary=False,\n",
    "    devices=8\n",
    ")\n",
    "dummy_input = torch.randn(1, projection_data.shape[1]).to(mlcv_model.device)\n",
    "traced_model = torch.jit.trace(mlcv_model, dummy_input)\n",
    "traced_model.save(f\"/home/shpark/prj-mlcv/lib/bioemu/model/{date}/mlcv_model-jit.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcf889c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=MLCV\n",
      "  (norm_in): RecursiveScriptModule(original_name=Normalization)\n",
      "  (encoder): RecursiveScriptModule(\n",
      "    original_name=FeedForward\n",
      "    (nn): RecursiveScriptModule(\n",
      "      original_name=Sequential\n",
      "      (0): RecursiveScriptModule(original_name=Linear)\n",
      "      (1): RecursiveScriptModule(original_name=Tanh)\n",
      "      (2): RecursiveScriptModule(original_name=Dropout)\n",
      "      (3): RecursiveScriptModule(original_name=Linear)\n",
      "      (4): RecursiveScriptModule(original_name=Tanh)\n",
      "      (5): RecursiveScriptModule(original_name=Dropout)\n",
      "      (6): RecursiveScriptModule(original_name=Linear)\n",
      "      (7): RecursiveScriptModule(original_name=Dropout)\n",
      "    )\n",
      "  )\n",
      "  (postprocessing): RecursiveScriptModule(original_name=PostProcess)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "jit_loaded = torch.jit.load(f\"/home/shpark/prj-mlcv/lib/bioemu/model/{date}/mlcv_model-jit.pt\")\n",
    "print(jit_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b601835b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:03<00:00, 15.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final CV shape: (534743, 1)\n",
      "CV value range: -1.0000 to 1.0000\n",
      "CV mean: 0.5073, std: 0.7848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cv_from_jit = np.zeros((cv_values.shape[0], 1))  # Shape: (534743, 1)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for batch_idx, (batch_data,) in enumerate(tqdm(dataloader)):\n",
    "        batch_cv = jit_loaded(batch_data)\n",
    "        batch_cv_np = batch_cv.detach().cpu().numpy()\n",
    "        cv_from_jit[batch_idx * batch_size:(batch_idx + 1) * batch_size] = batch_cv_np\n",
    "\n",
    "print(f\"\\nFinal CV shape: {cv_from_jit.shape}\")\n",
    "print(f\"CV value range: {cv_from_jit.min().item():.4f} to {cv_from_jit.max().item():.4f}\")\n",
    "print(f\"CV mean: {cv_from_jit.mean().item():.4f}, std: {cv_from_jit.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881673d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioemu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
